{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from torchvision.datasets.kinetics import Kinetics400\n",
    "\n",
    "from IPython.display import Video\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "Path.ls = lambda x: [o.name for o in x.iterdir()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Video Transformations from [`pytorch/vision/references/video_classification/transforms.py` ](https://github.com/pytorch/vision/blob/master/references/video_classification/transforms.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "def crop(vid, i, j, h, w):\n",
    "    return vid[..., i:(i + h), j:(j + w)]\n",
    "\n",
    "\n",
    "def center_crop(vid, output_size):\n",
    "    h, w = vid.shape[-2:]\n",
    "    th, tw = output_size\n",
    "\n",
    "    i = int(round((h - th) / 2.))\n",
    "    j = int(round((w - tw) / 2.))\n",
    "    return crop(vid, i, j, th, tw)\n",
    "\n",
    "\n",
    "def hflip(vid):\n",
    "    return vid.flip(dims=(-1,))\n",
    "\n",
    "\n",
    "# NOTE: for those functions, which generally expect mini-batches, we keep them\n",
    "# as non-minibatch so that they are applied as if they were 4d (thus image).\n",
    "# this way, we only apply the transformation in the spatial domain\n",
    "def resize(vid, size, interpolation='bilinear'):\n",
    "    # NOTE: using bilinear interpolation because we don't work on minibatches\n",
    "    # at this level\n",
    "    scale = None\n",
    "    if isinstance(size, int):\n",
    "        scale = float(size) / min(vid.shape[-2:])\n",
    "        size = None\n",
    "    return torch.nn.functional.interpolate(\n",
    "        vid, size=size, scale_factor=scale, mode=interpolation, align_corners=False)\n",
    "\n",
    "\n",
    "def pad(vid, padding, fill=0, padding_mode=\"constant\"):\n",
    "    # NOTE: don't want to pad on temporal dimension, so let as non-batch\n",
    "    # (4d) before padding. This works as expected\n",
    "    return torch.nn.functional.pad(vid, padding, value=fill, mode=padding_mode)\n",
    "\n",
    "\n",
    "def to_normalized_float_tensor(vid):\n",
    "    return vid.permute(3, 0, 1, 2).to(torch.float32) / 255\n",
    "\n",
    "\n",
    "def normalize(vid, mean, std):\n",
    "    shape = (-1,) + (1,) * (vid.dim() - 1)\n",
    "    mean = torch.as_tensor(mean).reshape(shape)\n",
    "    std = torch.as_tensor(std).reshape(shape)\n",
    "    return (vid - mean) / std\n",
    "\n",
    "\n",
    "# Class interface\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(vid, output_size):\n",
    "        \"\"\"Get parameters for ``crop`` for a random crop.\n",
    "        \"\"\"\n",
    "        h, w = vid.shape[-2:]\n",
    "        th, tw = output_size\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "        i = random.randint(0, h - th)\n",
    "        j = random.randint(0, w - tw)\n",
    "        return i, j, th, tw\n",
    "\n",
    "    def __call__(self, vid):\n",
    "        i, j, h, w = self.get_params(vid, self.size)\n",
    "        return crop(vid, i, j, h, w)\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, vid):\n",
    "        return center_crop(vid, self.size)\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, vid):\n",
    "        return resize(vid, self.size)\n",
    "\n",
    "\n",
    "class ToFloatTensorInZeroOne(object):\n",
    "    def __call__(self, vid):\n",
    "        return to_normalized_float_tensor(vid)\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, vid):\n",
    "        return normalize(vid, self.mean, self.std)\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, vid):\n",
    "        if random.random() < self.p:\n",
    "            return hflip(vid)\n",
    "        return vid\n",
    "\n",
    "\n",
    "class Pad(object):\n",
    "    def __init__(self, padding, fill=0):\n",
    "        self.padding = padding\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, vid):\n",
    "        return pad(vid, self.padding, self.fill)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_dir = Path('/Users/rahulsomani/01_github_projects/video-classification/')\n",
    "data_dir = base_dir/'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!tree {data_dir/'train'}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "from torchvision.io.video import read_video\n",
    "from functools import partial as partial\n",
    "\n",
    "read_video = partial(read_video, pts_unit='sec')\n",
    "\n",
    "torchvision.io.read_video = partial(torchvision.io.read_video, pts_unit = 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = Kinetics400(data_dir/'train',\n",
    "                   step_between_clips = 1,\n",
    "                   extensions         = ('mp4',),\n",
    "                   frames_per_clip    = 32,\n",
    "                   frame_rate = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Seeing the Data to Understand What Exactly is in the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_metadata(data):\n",
    "    \"\"\"\n",
    "    Takes in a `Kinetics400` dataset, computes the no. of frames in each data point\n",
    "    and returns in a DataFrame\n",
    "    \"\"\"\n",
    "    fnames     = [f.rsplit('/')[-1] for f in data.metadata['video_paths']]\n",
    "    num_frames = [len(pts) for pts in data.metadata['video_pts']]\n",
    "    fps        = [i for i in data.metadata['video_fps']]\n",
    "\n",
    "    metadata = pd.DataFrame(list(zip(fnames, num_frames, fps)), columns = ['Filename', '# Frames', 'FPS'])\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_img(sub_plt, x, i):\n",
    "    \"\"\"\n",
    "    Where `x` is one data sample of shape `(T, H, W, C)` and\n",
    "    `i` is the index of `T` that must be plotted.\n",
    "    `sub_plt` is an `AxesSubplot`.\n",
    "    This function removes axes labels and ticks, and names the\n",
    "    subplot with the appropriate frame number\n",
    "    \"\"\"\n",
    "    sub_plt.imshow(x[i])\n",
    "    sub_plt.set_title(f'Frame #{i+1}')\n",
    "    sub_plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_adjacent(x, figsize=(8,5), i1=0, i2=31):\n",
    "    f, plots = plt.subplots(1, 2, figsize=figsize)\n",
    "    plots[0] = plot_img(plots[0], x, i1)\n",
    "    plots[1] = plot_img(plots[1], x, i2)\n",
    "\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Seeing the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<br>\n",
    "When the dataset of class `Kinetics400` is constructed with `frames_per_clip = 32`, it doesn't take into consideration any data points which have less than 32 clips. <br>\n",
    "\n",
    "As seen in the DataFrame above, `c2-sample1` and `c2-sample2` had less than 32 frames, and `[len(x) for x in data.video_clips.clips]` reveals that 0 samples were taken from both these clips. \n",
    "\n",
    "For `c1-sample1`, which has exactly 32 clips, one clip gets created, whereas for `c1-sample2`, 16 clips get created, leading to a dataset of `len = 17`. The dataset has 1 sample from `c1-sample1` and 16 samples from `c1-sample2`.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_metadata(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[len(x) for x in data.video_clips.clips]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "<h3> Below are the actual videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Video('../data/train/class1/c1-sample1.mp4', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Video('../data/train/class1/c1-sample2.mp4', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Video('../data/train/class2/c2-sample1.mp4', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Video('../data/train/class2/c2-sample2.mp4', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<br>\n",
    "\n",
    "To fully understand what's happening, look at the first and last frames of selected data samples (indices 0, 1, 2 and 16) below.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_adjacent(data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_adjacent(data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_adjacent(data[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_adjacent(data[16][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentations -- Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "import albumentations as A\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Read Video as `Torch Tensor`\n",
    "\n",
    "This isn't nearly as efficient as `torchvision`'s video reader but is useful to quickly read in a video as a `torch.Tensor` for experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "file = '/Users/rahulsomani/01_github_projects/video-classification/data/train/class1/c1-sample1.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def read_video_tensor(file):\n",
    "    import ffmpeg\n",
    "\n",
    "    out, _ = (\n",
    "        ffmpeg\n",
    "        .input(file)\n",
    "        .output('pipe:', format='rawvideo', pix_fmt='rgb24')\n",
    "        .run(capture_stdout=True)\n",
    "    )\n",
    "\n",
    "    meta = ffmpeg.probe(file)['streams'][0]\n",
    "    height, width = meta['height'], meta['width']\n",
    "\n",
    "    vid = (\n",
    "        np\n",
    "        .frombuffer(out, np.uint8)\n",
    "        .reshape([-1, height, width, 3])\n",
    "    )\n",
    "\n",
    "    return torch.from_numpy(vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vid = read_video_tensor(file)\n",
    "vid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Albumentations on a Single Frame\n",
    "\n",
    "<br>\n",
    "\n",
    "`x` is one data sample i.e. a `tensor` of 32 frames/images. To apply albumentations, we'll first apply it on one single frame, then loop and apply over all the frames. As seen below, _not all_ `albumentations` can be applied to `torch.Tensor`s, so they first need to be converted to `np.array`. However, this is a good thing because the `albumentations` always work faster on `np.array` vs. `torch.Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = data[3][0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show(aug, img):\n",
    "    img = aug(image = img)['image']\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "random.seed(42)\n",
    "show(A.ChannelShuffle(p=1), np.asarray(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in x: print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "random.seed(42)\n",
    "show(A.ChannelShuffle(p=1), x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "random.seed(42)\n",
    "show(A.ChannelShuffle(p=1), np.asarray(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "random.seed(42)\n",
    "show(A.ToGray(p=1), x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "random.seed(42)\n",
    "show(A.ToGray(p=1), np.asarray(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Albumentations on a Video i.e. List of Frames\n",
    "\n",
    "<br>\n",
    "\n",
    "Here, I define a function `aug_video` which applies a list of `albumentations` to a video. It ensures that the exact same transformation is applied to each frame of the video. This is a must because, for instance, you wouldn't want one frame of the video to be horizontally flipped while the next not. This is done using a constant `random.seed()` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/Users/rahulsomani/Desktop/tennis-dataset-1-minimal/train/forehand/point#1_shot#2_David_Ferrer_-_Best-Ever_Roadrunner_Points.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, _ = (\n",
    "    ffmpeg\n",
    "    .input(file)\n",
    "    .output('pipe:', format='rawvideo', pix_fmt='rgb24')\n",
    "    .run(capture_stdout=True)\n",
    ")\n",
    "\n",
    "meta = ffmpeg.probe(file)['streams'][1]\n",
    "height, width = meta['height'], meta['width']\n",
    "\n",
    "vid = (\n",
    "    np\n",
    "    .frombuffer(out, np.uint8)\n",
    "    .reshape([-1, height, width, 3])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = A.Compose([\n",
    "    A.HorizontalFlip(p=0.0),\n",
    "    #A.ToGray(p=1),\n",
    "    #A.CLAHE(p=1),\n",
    "    A.Cutout(p=1),\n",
    "    A.RandomRain(p=1),\n",
    "    #A.ChannelDropout(p=1),\n",
    "    #A.ChannelShuffle(p=1),\n",
    "    #A.InvertImg(p=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_video(vid, tfms):\n",
    "    seed = random.randint(0,99999)\n",
    "    aug_vid = []\n",
    "    for x in vid:\n",
    "        random.seed(seed)\n",
    "        aug_vid.append((tfms(image = np.asarray(x)))['image'])\n",
    "    return torch.from_numpy(np.stack(aug_vid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_adjacent(vid, figsize=(16, 9), i1=10, i2=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidaug = (aug_video(vid, tfms))\n",
    "vidaug.shape\n",
    "plot_adjacent(vidaug, figsize=(16, 9), i1=10, i2=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToFloatTensorInZeroOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Testing to see if the `ToFloatTensorInZeroOne` and `Normalize` from the torchvision Video Transforms can be incorporated in the same `Compose` list as `albumentations`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = torch.from_numpy(vid)\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "to_normalized_float_tensor(tmp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp.shape[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def normalize(vid, mean, std):\n",
    "    shape = (-1,) + (1,) * (vid.dim() - 1)\n",
    "    mean = torch.as_tensor(mean).reshape(shape)\n",
    "    std = torch.as_tensor(std).reshape(shape)\n",
    "    return (vid - mean) / std\n",
    "\n",
    "mean = [0.43216, 0.394666, 0.37645]\n",
    "std  = [0.22803, 0.22145, 0.216989]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shape = (-1,) + (1,) * (tmp.dim() - 1)\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.as_tensor(mean).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# in the official code\n",
    "(torch.as_tensor(mean).reshape(shape)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# recreating to see if same result can be optained with `C` axis at end instead of beginning\n",
    "torch.as_tensor(mean)[..., None, None, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.as_tensor(mean)[None, None, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_shape = (tmp/255. - torch.as_tensor(mean)[None, None, None])\n",
    "raw_shape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_target = to_normalized_float_tensor(tmp) - torch.as_tensor(mean).reshape(shape)\n",
    "test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "to_normalized_float_tensor(raw_shape).shape == test_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "to_normalized_float_tensor(raw_shape) == test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Torchvision Normalising and Albumentations\n",
    "\n",
    "<br>\n",
    "\n",
    "**They can't** be combined in one `Compose`(or it's too painful to make it happen). <br>\n",
    "Instead, in the following chunk of `__getitem__` function of `Kinetics400`...\n",
    "\n",
    "```python\n",
    "if self.transform is not None:\n",
    "    video = self.transform(video) \n",
    "```\n",
    "\n",
    "... we can pass in only the normalisation functions, and then add another line to call the `albumentations` transformations, like so:\n",
    "\n",
    "```python\n",
    "if self.transform is not None:\n",
    "    video = self.transform(video) # Torchvision `ToFloatTensorInZeroOne` and `Normalize`. Returns shape (C,T,H,W)\n",
    "    video = self.tfms_albumentations(video) # Albumentations Transforms\n",
    "```\n",
    "\n",
    "In order to do so, we'll need to tweak the `aug_video` function defined in the above section to `permute` to the appropriate shape `(T,H,W,C)`, and after the `albumentations` transforms are done, `permute` back to shape `(C,T,H,W)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms_torch = torchvision.transforms.Compose([\n",
    "    ToFloatTensorInZeroOne(),\n",
    "    Normalize(mean=[0.43216, 0.394666, 0.37645],\n",
    "              std=[0.22803, 0.22145, 0.216989])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms_torch(vidaug).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms_torch(vidaug).permute(1,2,3,0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine `aug_video` for combining both kinds of transformations -- accomodate changing sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_video(vid, tfms):\n",
    "    seed = random.randint(0,99999)\n",
    "    vid = vid.permute(1,2,3,0) # added line of code\n",
    "    aug_vid = []\n",
    "    for x in vid:\n",
    "        random.seed(seed)\n",
    "        aug_vid.append((tfms(image = np.asarray(x)))['image'])\n",
    "    return torch.from_numpy(np.stack(aug_vid)).permute(3,0,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tfms(vid, tfms_torch, tfms_albu):\n",
    "    if not isinstance(vid, torch.Tensor): vid = torch.from_numpy(vid)\n",
    "    return aug_video(tfms_torch(vid), tfms_albu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms_albu = A.Compose([\n",
    "    A.HorizontalFlip(p=1),\n",
    "    A.ChannelShuffle(p=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combine_tfms(vid, tfms_torch, tfms_albu)\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_adjacent(combined.permute(1,2,3,0), figsize=(16,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
